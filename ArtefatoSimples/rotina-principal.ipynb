{
    "cells": [
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": ["!pip install -r requirements.txt --user\n"]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["# Inicialização"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "from bbmagic import Spark, Hdfs, Kinit\n",
                "import os\n",
                "import getpass\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": ["!kdestroy\n"]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["# Parâmetros sessão Spark"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "#CONSTANTES PARA DEFINIR A QUAL CLUSTER SE CONECTAR E O NOME DA SESSAO NO CONTEXTO DO NOTEBOOK E QUAL LINGUAGEM A SER UTILIZADA\n",
                "HDP_VERSION = 3  # type: int\n",
                "SESSION_NAME = \"carga_sbx_d4q0b8f\"  # type: str\n",
                "LANGUAGE = \"python\"\n",
                "\n",
                "#CONSTANTES QUE DEFINEM OS VALORES PARA DIMENSIONAMENTO DA SESSAO SPARK NO YARN\n",
                "DRIVER_MEMORY = \"2g\"  # type: str\n",
                "DRIVER_CORES = 2  # type: int\n",
                "EXECUTOR_CORES = 3  # type: int\n",
                "EXECUTOR_MEMORY = \"6g\"  # type: str\n",
                "NUM_EXECUTORS = 1  # type: int \n",
                "\n",
                "#CONSTANTE QUE SERVE PARA CONFIGURAR PARAMETROSA DO SPARK CONFIG\n",
                "SPARK_CONF = {\n",
                "  \"spark.driver.memoryOverhead\": \"1G\",\n",
                "  \"spark.executor.memoryOverhead\": \"1G\",\n",
                "  \"spark.sql.broadcastTimeout\" : \"6000000\"\n",
                "}  # type: Dict[str, str]\n",
                "\n",
                "#CONSTANTE QUE SERVE PARA DEFINIR PARAMETROS A SEREM UTILIZADOS CONFORME AMBIENTE (MODELAGEM OU SCORAGE/PRODUCAO) NO QUAL O JOB SERA EXECUTADO \n",
                "AMBIENTE = os.environ[\"AMBIENTE\"] \n",
                "if AMBIENTE == \"MODELAGEM\":\n",
                "  ENV_VAR = {\n",
                "    \"DOMINIO\":\"sbx_d4q0b8f\",\n",
                "    \"KEYTAB\": input(\"Matrícula SISBB:\"),\n",
                "}\n",
                "else:\n",
                "  ENV_VAR = {\n",
                "    \"DOMINIO\":\"hive_d4q\",\n",
                "    \"KEYTAB\": os.environ['KEYTAB']\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["# Cria sessão Spark"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "spark = Spark(\n",
                "  session_name= SESSION_NAME, \n",
                "  username=  ENV_VAR[\"KEYTAB\"], \n",
                "  hdp = HDP_VERSION, \n",
                "  language = LANGUAGE, \n",
                "  executor_memory=EXECUTOR_MEMORY,\n",
                "  spark_conf=SPARK_CONF,\n",
                "  auth = 'Kerberos',\n",
                "  env = ENV_VAR\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["# Importacoes"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%spark\n",
                "\n",
                "import os\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["# Tabelas"]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["##  *** opr_cred_sum ***"]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["### Sql"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%spark\n",
                "sql1 = '''\n",
                "SELECT --DISTINCT\n",
                "1 as cd_ogn,\n",
                "DT_REF, --DT_MVTC = DT_REF\n",
                "CD_ABTR_LIM_NOVO, --CD_RAS_2022,\n",
                "sum(VL_SDO_CTB) as VL_SDO_CTB,\n",
                "sum(VL_PVS_CLCD_CTB) as VL_PVS_CLCD_CTB,\n",
                "sum(VL_EXPO_ABGT) as VL_EXPO_ABGT, --VL_EXPO_ALT = VL_EXPO_ABGT\n",
                "sum(VL_EXPO_PDRD_RSCO) as VL_EXPO_PDRD_RSCO, --VL_EXPO_PDRD_RSCO_ALT = VL_EXPO_PDRD_RSCO\n",
                "sum(VL_PCL_PDRD_RSCO) as VL_PCL_PDRD_RSCO --VL_PCL_PDRD_RSCO_ALT = VL_PCL_PDRD_RSCO\n",
                "FROM hive_d4q.cptl_rglo_msl\n",
                "WHERE CD_IDFR_RBC_VLDO = 0 --IDFR_RBC_CTB_VLDO = CD_IDFR_RBC_VLDO\n",
                "--AND dt_ref = '2021-12-31'\n",
                "AND CD_QLF_TIP_OPR = 1\n",
                "GROUP BY DT_REF, CD_ABTR_LIM_NOVO --, CD_RAS_2022\n",
                "''' \n\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%spark\n",
                "print(sql1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["### Dataframe"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "%%spark\n",
                "\n",
                "df1 = spark.sql(sql1).coalesce(20)\n",
                "#dfData1 = df1.groupBy('aa_ref','mm_ref').count()\n",
                "#dfData1.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["### Nome da tabela"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%spark\n",
                "\n",
                "tabela1 = os.environ[\"DOMINIO\"] + \".opr_cred_sum\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["### Comando de exclusão das partições"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "%%spark\n",
                "\n",
                "#dfData1.cache()\n",
                "#mes = int(dfData1.first()['mm_ref'])\n",
                "#ano = int(dfData1.first()['aa_ref'])\n",
                "#sqlParticao = \"ALTER TABLE \" + tabela1 + \" DROP IF EXISTS PARTITION(AA_REF= \" + str(ano) + \", MM_REF = \" + str(mes) + \")\"\n",
                "#sqlParticao10 = \"ALTER TABLE \" + tabela1 + \" DROP IF EXISTS PARTITION(AA_REF= \" + str(ano-10) + \", MM_REF = \" + str(mes) + \")\"\n",
                "#print(sqlParticao)\n",
                "#print(sqlParticao10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["### Execução de exclusão das partições"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "%%spark\n",
                "\n",
                "#spark.sql(sqlParticao)\n",
                "#spark.sql(sqlParticao10)\n",
                "#spark.catalog.refreshTable(tabela1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["### Estrutura de fonte e destino"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%spark\n",
                "\n",
                "#dfDestino = spark.sql(\"select * from \" + tabela1 + \" limit 1\")\n",
                "#df1.printSchema()\n",
                "#dfDestino.printSchema()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["### Ordenação de campos. Totais antes e depois."]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%spark\n",
                "\n",
                "#dfOrdenada = df1.select(dfDestino.columns)\n",
                "\n",
                "#dfOrdenada.groupBy('aa_ref','mm_ref').count().show()\n",
                "#df1.groupBy('aa_ref','mm_ref').count().show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["### Inserindo dados na tabela"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "%%spark\n",
                "\n",
                "#dfOrdenada.write.insertInto(tabela1)\n",
                "df1.write.mode(\"overwrite\").saveAsTable(tabela1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["### Total da tabela"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%spark\n",
                "\n",
                "#print(tabela1)\n",
                "#dfResult = spark.sql(\"select * from \" + tabela1)\n",
                "#dfResult.groupBy('aa_ref','mm_ref').count().show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": ["## Finaliza sessão Spark"]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "source": ["%spark cleanup\n"]
        }
    ],
    "metadata": {
        "language_info": {"name": " "},
        "kernelspec": {
            "name": "python3",
            "language": "python",
            "display_name": "Python 3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}